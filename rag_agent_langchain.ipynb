{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414641bf",
   "metadata": {},
   "source": [
    "# Local Semantic Search & RAG with Qdrant and Ollama\n",
    "\n",
    "This notebook demonstrates a **local-first Retrieval-Augmented Generation (RAG) pipeline**.\n",
    "\n",
    "**High-level flow**\n",
    "1. Raw text documents are converted into vector embeddings using a local Ollama embedding model.\n",
    "2. Embeddings are stored in Qdrant, a vector database optimized for similarity search.\n",
    "3. User queries are embedded using the same model.\n",
    "4. Qdrant retrieves the most semantically similar documents.\n",
    "5. (Optional) Retrieved context is passed to a local LLM via Ollama to generate grounded answers.\n",
    "\n",
    "The entire stack runs locally, with no external API dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383fb177-25a4-4682-adf8-bb2cd043d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bs4 langchain dotenv langchain-community langchain-text-splitters langchain-ollama langchain-qdrant qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbb9dd",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are dense numerical representations of text that capture semantic meaning.\n",
    "They are **mandatory** for semantic search: vector databases such as Qdrant operate on vectors,\n",
    "not raw text.\n",
    "\n",
    "Key points:\n",
    "- The same embedding model must be used for both documents and queries.\n",
    "- Vector dimensionality must match the Qdrant collection configuration.\n",
    "- Small changes in text should produce small changes in vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a208804-49c6-494f-8f8a-de03c145dee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.agents import create_agent\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "849f4b7e-0c6e-4047-94b1-509c00823633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d561ed48-1ac8-404b-b6a7-6da653fabcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\", \n",
    "              \"https://github.com/NirDiamant/Prompt_Engineering\"),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "from uuid import uuid4\n",
    "\n",
    "for doc in docs:\n",
    "    doc.metadata[\"doc_id\"] = str(uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16822c1b-9966-432a-aaa5-4803771d4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0660699a-f896-40aa-961a-968f0c896b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'doc_id': 'e6094d18-bad0-4f60-8a11-4ba36cc84363', 'start_index': 15618}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64b9a0b-3fab-4176-84fd-1034e8c79b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"embeddinggemma\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0571bf-5311-4235-b604-99df0ab5db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "vector = embeddings.embed_query(\"Hello world\")\n",
    "\n",
    "print(len(vector)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bed5b9",
   "metadata": {},
   "source": [
    "## Qdrant Collection\n",
    "\n",
    "A Qdrant *collection* is analogous to a table in a relational database.\n",
    "\n",
    "It defines:\n",
    "- Vector size (must match the embedding model output dimension)\n",
    "- Distance metric (e.g. cosine similarity)\n",
    "- Optional payload schema for metadata filtering\n",
    "\n",
    "In this notebook, the collection is created once and reused across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891bc07f-0da3-4897-b050-d5ee59a3e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Qdrant client (local)\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"http://localhost:6333\",\n",
    ")\n",
    "\n",
    "# Create / load collection\n",
    "collection_name = \"github_ai_repo\"\n",
    "\n",
    "\n",
    "# vectorstore = QdrantVectorStore.from_documents(\n",
    "#    documents=splits,\n",
    "#    embedding=embeddings,\n",
    "#    url=\"http://localhost:6333\",\n",
    "#    collection_name=collection_name,\n",
    "# )\n",
    "\n",
    "vectorstore = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=embeddings,\n",
    "    url=\"http://localhost:6333\",\n",
    "    collection_name=collection_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f05e1",
   "metadata": {},
   "source": [
    "## Semantic Search\n",
    "\n",
    "Semantic search works by:\n",
    "1. Embedding the user query.\n",
    "2. Computing similarity between the query vector and stored vectors.\n",
    "3. Returning the closest matches according to the distance metric.\n",
    "\n",
    "Unlike keyword search, this retrieves documents based on meaning rather than exact terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6507bf-d5f9-4c1c-bb2f-478ce27918d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac73e1df-4cf9-4247-ab18-c5c09b2b2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"qwen3:0.6b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74cba5f8-1c8f-4ce4-8b37-b24b27753c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3292862-4c64-41ba-ae33-87e830cd5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Prompt Engineering?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278956c-3c4a-4485-96d4-4eec7e0639cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
